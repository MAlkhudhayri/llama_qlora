{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/data/zoo/llama2/llama2-7b-hf/')\n",
    "batch_size = 4\n",
    "input_ids = tokenizer.encode('os:', return_tensors='pt')\n",
    "# tokenizer(\n",
    "#     # batch_size*[''],\n",
    "#     ['4235 ', '5462 ', '7132 ', '6460 '], \n",
    "#     return_tensors=\"pt\"\n",
    "# ).input_ids  # Batch size 1\n",
    "\n",
    "def sample(ckpt_dir, step):\n",
    "    output_dir = os.path.join(ckpt_dir, f'checkpoint-{step}') #f'/mnt/data/sonia/ckpts/llama-2-7b-jul25/checkpoint-{step}/'\n",
    "    print(output_dir)\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model = model.merge_and_unload() #returns type transformers.models.llama.modeling_llama.LlamaForCausalLM\n",
    "    \n",
    "    outputs = model.generate(input_ids, max_new_tokens=100).cpu()\n",
    "    out=tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    model = model.cpu()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/ckpts/debug/checkpoint-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111291ed34e24458bd92dd3576a42a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['os:\\n\\n    # 1. 설정 파일 생성\\n    # 2. 설정 파일 읽어오기\\n    # 3. 설정 파일 쓰기\\n    # 4. 설정 파일 삭제\\n    # 5. 설정 �']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sample('/mnt/data/sonia/ckpts/debug', 20)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os:\n",
      "1 {'os': 'Windows Server 2012 R2 Datacenter 9600', 'ip_str': '10.10.10.10', 'port': 445, 'module': 'smb', 'cpe': None, 'cpe_count': 0, 'category': 'file_sharing', 'os_generic': 'windows server', 'single_cpe': None}\n"
     ]
    }
   ],
   "source": [
    "print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1e-4, wd 1e-3, epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2636d67efab64ba9898ebf8735c0fd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4235.0 - Population by Age and Sex, Regions of Australia, 2011\n",
      "Population by Age and Sex, Regions of Australia, 2011\n",
      "Population by Age and Sex, Regions of Australia, 2011 (cat. no. 4235.0) was released on 20 December 2012.\n",
      "The ABS has released a new publication, Population by Age and Sex, Regions of\n",
      "5462.5000 - 5462.5000\n",
      "5462.5000 - 5462.5000 - 5462.5000\n",
      "5462.5000 - 5462.5000 - 5462.5000\n",
      "5462.5000 - 5462.500\n",
      "7132.0 - Australian National Accounts: National Income, Expenditure and Product, Jun 2018 Quality Declaration\n",
      "Australian National Accounts: National Income, Expenditure and Product, Jun 2018\n",
      "The Australian National Accounts: National Income, Expenditure and Product, Jun 2018 (cat. no. 5206.0) publication presents estimates of the value of goods and services produced in Australia\n",
      "6460000000000000000♠0.0000000000000000000♠0.000000000000000000♠0.000000000000000000♠0.000000000000000000\n",
      "\n",
      "lr 1e-4, wd 1e-3, epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088c5653817c441885e559244b65a11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4235.0 - Population by Age and Sex, Regions of Australia, 2011\n",
      "Population by Age and Sex, Regions of Australia, 2011\n",
      "Population by Age and Sex, Regions of Australia, 2011 (cat. no. 4235.0) was released on 29 May 2013.\n",
      "The ABS has released a new publication, Population by Age and Sex, Regions of\n",
      "5462.5000 - 5462.5000\n",
      "5462.5000 - 5462.5000 - 5462.5000\n",
      "5462.5000 - 5462.5000 - 5462.5000\n",
      "5462.5000 - 5462.500\n",
      "7132.0 - Australian National Accounts: National Income, Expenditure and Product, Jun 2017 Quality Declaration\n",
      "Australian National Accounts: National Income, Expenditure and Product, Jun 2017\n",
      "The Australian National Accounts: National Income, Expenditure and Product (cat. no. 5206.0) is a comprehensive and detailed quarterly statistical publication which presents estimates of the economic performance of the Australian\n",
      "6460000000000000000♠0.0000000000000000000♠0.000000000000000000♠0.000000000000000000♠0.000000000000000000\n",
      "\n",
      "lr 1e-4, wd 1e-3, epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f43de183cf4f4ca0b7b32c56a3b4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr 1e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, wd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m ckptdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/sonia/ckpts/grid/lr1e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_wd\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/adapter_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m sample(i, ckptdir):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36msample\u001b[0;34m(step, output_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(step, output_dir):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# output_dir = f'/mnt/data/sonia/ckpts/llama-2-7b-nov12/checkpoint-{step}/adapter_model'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoPeftModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(output_dir, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload() \u001b[38;5;66;03m#returns type transformers.models.llama.modeling_llama.LlamaForCausalLM\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m.\u001b[39mcuda(), max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     15\u001b[0m     out\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/peft/tuners/lora.py:618\u001b[0m, in \u001b[0;36mLoraModel.merge_and_unload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_and_unload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    602\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m    This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m    as a standalone model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unload_and_optionally_merge()\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/peft/tuners/lora.py:465\u001b[0m, in \u001b[0;36mLoraModel._unload_and_optionally_merge\u001b[0;34m(self, merge)\u001b[0m\n\u001b[1;32m    463\u001b[0m         new_module \u001b[38;5;241m=\u001b[39m Conv1D(target\u001b[38;5;241m.\u001b[39mout_features, target\u001b[38;5;241m.\u001b[39min_features)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m         new_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(target\u001b[38;5;241m.\u001b[39min_features, target\u001b[38;5;241m.\u001b[39mout_features, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge:\n\u001b[1;32m    467\u001b[0m     target\u001b[38;5;241m.\u001b[39mmerge()\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[38;5;241m.\u001b[39mkaiming_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, a\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;241m-\u001b[39mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in [-4,-6,-8]:\n",
    "    for wd in ['1e-3', '1e-4', '1e-5']:\n",
    "        for i in range(10,22,2):\n",
    "            print(f'lr 1e{lr}, wd {wd}, epoch {i}')\n",
    "            ckptdir = f'/mnt/data/sonia/ckpts/grid/lr1e{lr}_wd{wd}/checkpoint-{i}/adapter_model'\n",
    "            for out in sample(i, ckptdir):\n",
    "                print(out)\n",
    "            print('')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 29871, 29946, 29906, 29941, 29945, 29871],\n",
       "        [    1, 29871, 29945, 29946, 29953, 29906, 29871],\n",
       "        [    1, 29871, 29955, 29896, 29941, 29906, 29871],\n",
       "        [    1, 29871, 29953, 29946, 29953, 29900, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/data/zoo/llama2/llama2-7b-hf/')\n",
    "batch_size = 4\n",
    "input = tokenizer(\n",
    "    # batch_size*[''],\n",
    "    ['4235 ', '5462 ', '7132 ', '6460 '], \n",
    "    return_tensors=\"pt\"\n",
    ")  # Batch size 1\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/ckpts/debug/checkpoint-20/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf09188fbd34c7bb75ec2aa7fa247d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step=20\n",
    "output_dir = f'/mnt/data/sonia/ckpts/debug/checkpoint-{step}/'\n",
    "print(output_dir)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# model=model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"4235  {'os': 'Windows Server 2008 R2 Datacenter 7601 Service Pack 1', 'ip_str': '10.1.1.1', 'port': 445, 'module': 'smb', 'cpe': None, 'cpe_count': 0, 'category': 'file_sharing', 'os_generic': 'windows server', 'single_cpe\",\n",
       " \"5462  {'os': 'Windows Server 2008 R2 Datacenter 7601 Service Pack 1', 'ip_str': '10.1.1.1', 'port': 445, 'module': 'smb', 'cpe': None, 'cpe_count': 0, 'category': 'file_sharing', 'os_generic': 'windows server', 'single_cpe\",\n",
       " \"7132  {'os': 'Windows Server 2008 R2 Datacenter 7601 Service Pack 1', 'ip_str': '10.1.1.1', 'port': 445, 'module': 'smb', 'cpe': None, 'cpe_count': 0, 'category': 'file_sharing', 'os_generic': 'windows server', 'single_cpe\",\n",
       " \"6460  {'os': 'Windows Server 2008 R2 Datacenter 7601 Service Pack 1', 'ip_str': '10.1.1.1', 'port': 445, 'module': 'smb', 'cpe': None, 'cpe_count': 0, 'category': 'file_sharing', 'os_generic': 'windows server', 'single_cpe\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**input, max_length=100)\n",
    "out=tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
